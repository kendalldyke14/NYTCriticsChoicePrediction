{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Complete output from command python setup.py egg_info:\n",
      "    ERROR: Traceback (most recent call last):\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\ruamel_yaml\\__init__.py\", line 21, in <module>\n",
      "        from .main import *                               # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\ruamel_yaml\\main.py\", line 12, in <module>\n",
      "        import ruamel.yaml\n",
      "    ModuleNotFoundError: No module named 'ruamel'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\setup.py\", line 14, in <module>\n",
      "        import ruamel_yaml  # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\ruamel_yaml\\__init__.py\", line 23, in <module>\n",
      "        from ruamel_yaml.main import *                               # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\ruamel_yaml\\main.py\", line 12, in <module>\n",
      "        import ruamel.yaml\n",
      "    ModuleNotFoundError: No module named 'ruamel'\n",
      "    ----------------------------------------\n",
      "ERROR: Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-4yud2msm\\ruamel-yaml-conda\\\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NYT Data for 2020 and 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt2020 = pd.read_csv(\"data/NYTData2020.csv\")\n",
    "nyt2021 = pd.read_csv(\"data/NYTData2021.csv\")\n",
    "\n",
    "nyt = nyt2021.append(nyt2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take NYT review summaries and train a Top2Vec Model\n",
    "\n",
    "https://pypi.org/project/top2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 19:38:04,730 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-01-10 19:38:05,084 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:absl:Using C:\\Users\\kenda\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "2022-01-10 19:38:22,116 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-01-10 19:38:23,597 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-01-10 19:38:52,473 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-01-10 19:38:52,640 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "documents = nyt['summary_short'].tolist()\n",
    "\n",
    "model = Top2Vec(documents, speed=\"deep-learn\", workers=8,\n",
    "                embedding_model = \"universal-sentence-encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Number of Topics Defined by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents are most similar to each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([930, 411,  55], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Top Words for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['comedy', 'drama', 'thriller', 'director', 'movie', 'film',\n",
       "        'documentary', 'this', 'netflix', 'into', 'that', 'plays', 'it',\n",
       "        'story', 'the', 'her', 'and', 'as', 'of', 'from', 'its', 'stars',\n",
       "        'but', 'family', 'is', 'with', 'about', 'for', 'an', 'to', 'in',\n",
       "        'feature', 'who', 'his', 'at', 'two', 'new', 'by', 'on',\n",
       "        'follows', 'young', 'are', 'woman', 'life', 'up', 'their'],\n",
       "       ['documentary', 'thriller', 'film', 'drama', 'that', 'this',\n",
       "        'movie', 'comedy', 'about', 'it', 'director', 'life', 'into',\n",
       "        'from', 'story', 'the', 'her', 'of', 'and', 'as', 'but',\n",
       "        'netflix', 'woman', 'who', 'feature', 'with', 'for', 'its', 'an',\n",
       "        'is', 'to', 'on', 'two', 'at', 'by', 'family', 'in', 'up', 'his',\n",
       "        'are', 'plays', 'new', 'stars', 'their', 'young', 'follows'],\n",
       "       ['documentary', 'netflix', 'movie', 'film', 'thriller', 'comedy',\n",
       "        'drama', 'her', 'director', 'this', 'that', 'it', 'as', 'from',\n",
       "        'the', 'its', 'of', 'about', 'feature', 'his', 'but', 'life',\n",
       "        'into', 'woman', 'on', 'with', 'is', 'and', 'for', 'to', 'at',\n",
       "        'story', 'by', 'their', 'an', 'in', 'up', 'two', 'are', 'who',\n",
       "        'stars', 'plays', 'new', 'family', 'young', 'follows']],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with OMDB Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in OMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb2020 = pd.read_csv(\"data/OMDBData2020.csv\")\n",
    "omdb2020.Plot = omdb2020.Plot.fillna(value=\"\")\n",
    "\n",
    "omdb2021 = pd.read_csv(\"data/OMDBData2021.csv\")\n",
    "omdb2021.Plot = omdb2021.Plot.fillna(value=\"\")\n",
    "\n",
    "omdb = omdb2020.append(omdb2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 5k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = (1,2), # include up to 2-grams, we can make this only 1 if needed\n",
    "                                       min_df=1,  # note: absolute count of doc\n",
    "                                       token_pattern = r'\\b[a-z]{3,12}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(omdb[\"Plot\"])\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train NMF for n = 2-10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:\n",
      "[['family', 'young', 'old', 'father', 'year', 'woman', 'year old', 'home', 'mother', 'finds'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'city', 'love']]\n",
      "\n",
      "\n",
      "3 Topics:\n",
      "[['family', 'young', 'old', 'father', 'woman', 'year', 'home', 'year old', 'mother', 'finds'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'city', 'love'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school football', 'school students', 'football']]\n",
      "\n",
      "\n",
      "4 Topics:\n",
      "[['family', 'young', 'father', 'woman', 'home', 'mother', 'son', 'wife', 'daughter', 'house'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'love', 'city'], ['school', 'high', 'high school', 'students', 'team', 'senior', 'student', 'school football', 'school students', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend']]\n",
      "\n",
      "\n",
      "5 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'daughter', 'house'], ['life', 'film', 'story', 'world', 'documentary', 'love', 'american', 'music', 'time', 'people'], ['school', 'high', 'high school', 'students', 'team', 'senior', 'student', 'school football', 'school students', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend'], ['new', 'york', 'new york', 'city', 'york city', 'right', 'swiped', 'time new', 'swiped right', 'based']]\n",
      "\n",
      "\n",
      "6 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'daughter', 'house'], ['story', 'film', 'world', 'love', 'documentary', 'american', 'time', 'follows', 'people', 'music'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school students', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend'], ['new', 'york', 'new york', 'city', 'york city', 'time new', 'swiped', 'swiped right', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth look', 'depth', 'songwriter', 'singer songwriter']]\n",
      "\n",
      "\n",
      "7 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'house', 'daughter'], ['world', 'film', 'story', 'documentary', 'american', 'time', 'follows', 'history', 'music', 'people'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school students', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'best', 'friend', 'thousand year'], ['new', 'york', 'new york', 'city', 'york city', 'swiped right', 'time new', 'swiped', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth', 'depth look', 'songwriter', 'singer songwriter'], ['love', 'films', 'short films', 'oscar nominated', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation']]\n",
      "\n",
      "\n",
      "8 Topics:\n",
      "[['family', 'father', 'young', 'mother', 'woman', 'home', 'son', 'wife', 'daughter', 'house'], ['world', 'film', 'story', 'documentary', 'american', 'time', 'music', 'follows', 'history', 'lives'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'school students', 'student', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'best', 'friend', 'thousand year'], ['new', 'york', 'new york', 'city', 'york city', 'swiped right', 'swiped', 'time new', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth', 'depth look', 'songwriter', 'singer songwriter'], ['love', 'films', 'short films', 'oscar nominated', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['town', 'small', 'small town', 'local', 'midwest', 'midwest town', 'moves', 'death', 'community', 'terrorizing']]\n",
      "\n",
      "\n",
      "9 Topics:\n",
      "[['world', 'town', 'fight', 'small', 'way', 'finds', 'mysterious', 'group', 'time', 'local'], ['film', 'story', 'documentary', 'american', 'music', 'history', 'black', 'people', 'follows', 'world'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'school students', 'school football', 'student', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'thousand year', 'thousand', 'love'], ['new', 'york', 'new york', 'city', 'york city', 'time new', 'swiped', 'swiped right', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'depth', 'depth look', 'singer', 'songwriter', 'singer songwriter'], ['love', 'short films', 'oscar nominated', 'films', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['family', 'home', 'brother', 'new', 'forced', 'trip', 'road', 'mother', 'takes', 'house'], ['young', 'father', 'woman', 'son', 'young woman', 'mother', 'wife', 'daughter', 'girl', 'house']]\n",
      "\n",
      "\n",
      "10 Topics:\n",
      "[['world', 'fight', 'way', 'war', 'time', 'finds', 'mysterious', 'group', 'lives', 'battle'], ['film', 'story', 'documentary', 'american', 'music', 'history', 'black', 'people', 'feature', 'follows'], ['school', 'high school', 'high', 'students', 'senior', 'team', 'school students', 'school football', 'student', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'thousand year', 'thousand', 'ground'], ['new', 'york', 'new york', 'city', 'york city', 'swiped', 'swiped right', 'time new', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'depth', 'depth look', 'singer', 'songwriter', 'singer songwriter'], ['love', 'short films', 'oscar nominated', 'films', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['family', 'home', 'brother', 'new', 'forced', 'trip', 'road', 'mother', 'takes', 'house'], ['young', 'father', 'woman', 'son', 'young woman', 'mother', 'daughter', 'wife', 'girl', 'house'], ['town', 'small', 'small town', 'midwest', 'local', 'midwest town', 'moves', 'community', 'terrorizing', 'home']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "for num_topics in range(2, 11):\n",
    "    nmf = NMF(\n",
    "        n_components=num_topics,\n",
    "        init='nndsvd', \n",
    "        random_state = 42\n",
    "        )\n",
    "    W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "    H = nmf.components_\n",
    "    top_components = np.argsort(-H)[:, :10]\n",
    "    \n",
    "    topics = []\n",
    "    for t in top_components:\n",
    "        topic_words = []\n",
    "        for w in t:\n",
    "            topic_words.append(feature_names_NMF[w])\n",
    "        topics.append(topic_words)\n",
    "        \n",
    "    print(f\"{num_topics} Topics:\")\n",
    "    print(topics)\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYT Full Review Text\n",
    "\n",
    "TFIDF and NMF on full NYT Review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTReviews = pd.read_csv(\"data/NYTData_wReviewText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 5k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = (1,2), # include up to 2-grams, we can make this only 1 if needed\n",
    "                                       min_df=1,  # note: absolute count of doc\n",
    "                                       token_pattern = r'\\b[a-z]{3,12}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(NYTReviews[\"review_text\"])\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:\n",
      "[['movie', 'like', 'story', 'film', 'character', 'time', 'characters', 'family', 'man', 'life'], ['documentary', 'film', 'says', 'footage', 'new', 'people', 'interviews', 'work', 'black', 'world']]\n",
      "\n",
      "\n",
      "3 Topics:\n",
      "[['movie', 'like', 'story', 'film', 'character', 'time', 'characters', 'family', 'man', 'life'], ['documentary', 'film', 'says', 'footage', 'people', 'new', 'interviews', 'black', 'work', 'history'], ['oscar', 'oscar nominated', 'nominated', 'live action', 'short', 'colette', 'program', 'program hour', 'hour program', 'action']]\n",
      "\n",
      "\n",
      "4 Topics:\n",
      "[['movie', 'like', 'story', 'character', 'characters', 'film', 'time', 'family', 'man', 'woman'], ['film', 'documentary', 'says', 'people', 'camera', 'history', 'subjects', 'world', 'women', 'interviews'], ['oscar', 'oscar nominated', 'nominated', 'live action', 'short', 'colette', 'program', 'hour program', 'program hour', 'action'], ['music', 'new', 'rock', 'york', 'new york', 'musical', 'artist', 'band', 'movie', 'documentary']]\n",
      "\n",
      "\n",
      "5 Topics:\n",
      "[['movie', 'woman', 'women', 'film', 'like', 'man', 'story', 'men', 'horror', 'young'], ['documentary', 'film', 'says', 'footage', 'interviews', 'people', 'new', 'history', 'american', 'black'], ['oscar', 'oscar nominated', 'nominated', 'colette', 'live action', 'short', 'program', 'program hour', 'hour program', 'hour'], ['movie', 'like', 'movies', 'time', 'comedy', 'just', 'played', 'character', 'good', 'directed'], ['family', 'father', 'children', 'mother', 'film', 'school', 'child', 'home', 'son', 'parents']]\n",
      "\n",
      "\n",
      "6 Topics:\n",
      "[['movie', 'like', 'movies', 'character', 'time', 'story', 'just', 'played', 'good', 'comedy'], ['documentary', 'film', 'says', 'people', 'history', 'states', 'united', 'interviews', 'subjects', 'footage'], ['colette', 'oscar', 'oscar nominated', 'nominated', 'burrow', 'bowers', 'latasha', 'year oscar', 'short', 'short films'], ['music', 'new', 'rock', 'artist', 'band', 'musical', 'film', 'york', 'new york', 'documentary'], ['film', 'family', 'mother', 'father', 'home', 'life', 'story', 'movie', 'children', 'woman'], ['program hour', 'hour program', 'hour', 'live action', 'program', 'english', 'live', 'action', 'oscar', 'oscar nominated']]\n",
      "\n",
      "\n",
      "7 Topics:\n",
      "[['movie', 'woman', 'women', 'film', 'like', 'man', 'story', 'men', 'young', 'horror'], ['documentary', 'film', 'says', 'footage', 'interviews', 'people', 'new', 'history', 'american', 'black'], ['colette', 'oscar', 'oscar nominated', 'nominated', 'burrow', 'bowers', 'latasha', 'year oscar', 'short', 'short films'], ['movie', 'like', 'played', 'time', 'character', 'comedy', 'just', 'good', 'directed', 'movies'], ['family', 'father', 'mother', 'film', 'children', 'school', 'home', 'child', 'son', 'parents'], ['deena', 'shadyside', 'fear street', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer'], ['program hour', 'hour program', 'hour', 'live action', 'program', 'english', 'live', 'action', 'oscar', 'oscar nominated']]\n",
      "\n",
      "\n",
      "8 Topics:\n",
      "[['woman', 'women', 'movie', 'film', 'like', 'man', 'story', 'men', 'horror', 'world'], ['documentary', 'film', 'says', 'footage', 'interviews', 'people', 'new', 'history', 'american', 'music'], ['colette', 'oscar', 'oscar nominated', 'nominated', 'latasha', 'bowers', 'burrow', 'year oscar', 'short', 'short films'], ['movie', 'like', 'played', 'character', 'comedy', 'time', 'just', 'good', 'directed', 'movies'], ['family', 'father', 'mother', 'home', 'children', 'son', 'child', 'film', 'life', 'drama'], ['school', 'high', 'high school', 'students', 'boys', 'kids', 'girls', 'film', 'student', 'girl'], ['program hour', 'hour program', 'hour', 'live action', 'program', 'english', 'live', 'action', 'oscar', 'oscar nominated'], ['deena', 'fear street', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer']]\n",
      "\n",
      "\n",
      "9 Topics:\n",
      "[['film', 'family', 'mother', 'father', 'home', 'life', 'movie', 'son', 'child', 'man'], ['documentary', 'film', 'says', 'people', 'history', 'footage', 'states', 'united', 'interviews', 'war'], ['colette', 'oscar', 'oscar nominated', 'nominated', 'burrow', 'latasha', 'bowers', 'year oscar', 'short', 'short films'], ['movie', 'like', 'character', 'movies', 'played', 'time', 'good', 'just', 'way', 'story'], ['music', 'rock', 'band', 'new', 'musical', 'singer', 'artist', 'documentary', 'songs', 'film'], ['school', 'high', 'high school', 'students', 'kids', 'boys', 'girls', 'film', 'children', 'student'], ['hour program', 'program hour', 'hour', 'live action', 'program', 'english', 'live', 'action', 'oscar nominated', 'oscar'], ['deena', 'fear street', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer'], ['women', 'men', 'woman', 'black', 'female', 'story', 'like', 'male', 'girls', 'white']]\n",
      "\n",
      "\n",
      "10 Topics:\n",
      "[['film', 'family', 'mother', 'father', 'life', 'home', 'movie', 'man', 'son', 'child'], ['documentary', 'film', 'says', 'people', 'history', 'footage', 'states', 'united', 'interviews', 'war'], ['colette', 'oscar', 'oscar nominated', 'nominated', 'burrow', 'latasha', 'bowers', 'year oscar', 'short', 'short films'], ['movie', 'like', 'character', 'movies', 'played', 'time', 'good', 'just', 'way', 'story'], ['music', 'rock', 'band', 'musical', 'singer', 'new', 'artist', 'documentary', 'songs', 'film'], ['school', 'high', 'high school', 'kids', 'students', 'boys', 'girls', 'film', 'children', 'student'], ['hour program', 'program hour', 'hour', 'live action', 'program', 'english', 'live', 'action', 'oscar nominated', 'oscar'], ['deena', 'shadyside', 'fear street', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer'], ['women', 'men', 'woman', 'black', 'female', 'story', 'like', 'male', 'girls', 'husband'], ['frank', 'philip', 'uncle frank', 'mangrove', 'uncle', 'beth', 'new york', 'york', 'hoffa', 'daphne']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "for num_topics in range(2, 11):\n",
    "    nmf = NMF(\n",
    "        n_components=num_topics,\n",
    "        init='nndsvd', \n",
    "        random_state = 42\n",
    "        )\n",
    "    W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "    H = nmf.components_\n",
    "    top_components = np.argsort(-H)[:, :10]\n",
    "    \n",
    "    topics = []\n",
    "    for t in top_components:\n",
    "        topic_words = []\n",
    "        for w in t:\n",
    "            topic_words.append(feature_names_NMF[w])\n",
    "        topics.append(topic_words)\n",
    "        \n",
    "    print(f\"{num_topics} Topics:\")\n",
    "    print(topics)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Topics into Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a tfidf bag of words vectorizer\n",
    "def tfidf_vecorization(data, ngram_range):\n",
    "    tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 20k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = ngram_range, # include up to 2-grams, we can make this only 1 if needed\n",
    "                                       min_df=1,  # note: absolute count of doc\n",
    "                                       token_pattern = r'\\b[a-z]{3,}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "    tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(data)\n",
    "    feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_documents_NMF, feature_names_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take TFIDF transformed data and train NMF\n",
    "def trainNMF(tfidf, n_components):\n",
    "    nmf = NMF(\n",
    "        n_components=n_components,\n",
    "        init='nndsvd', \n",
    "        random_state = 42\n",
    "        )\n",
    "    W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "    H = nmf.components_\n",
    "    top_components = np.argsort(-H)[:, :10]\n",
    "\n",
    "    topics = []\n",
    "    for t in top_components:\n",
    "        topic_words = []\n",
    "        for w in t:\n",
    "            topic_words.append(feature_names_NMF[w])\n",
    "        topics.append(topic_words)\n",
    "        \n",
    "    return W, H, topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"data/joined_df.csv\")\n",
    "movie_data.Plot = movie_data.Plot.fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign each movie a review topic\n",
    "\n",
    "Using the W matrix, we can see which topic is most related to that movie's review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_documents_NMF, feature_names_NMF = tfidf_vecorization(movie_data['review_text'], (1,2))\n",
    "W, H, topics = trainNMF(tfidf_documents_NMF, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W is the topic matrix, the highest value within each row will determine the most relevant topic\n",
    "movie_data[\"review_topic\"] = [x.argmax() for x in W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign each movie a plot topic\n",
    "\n",
    "Using the W matrix, we can see which topic is most related to that movie's plot. (Plot topics are different than review topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_documents_NMF, feature_names_NMF = tfidf_vecorization(movie_data['Plot'], (1,2))\n",
    "W, H, topics = trainNMF(tfidf_documents_NMF, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W is the topic matrix, the highest value within each row will determine the most relevant topic\n",
    "movie_data[\"plot_topic\"] = [x.argmax() for x in W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rated</th>\n",
       "      <th>review_topic</th>\n",
       "      <th>plot_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tenet</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dune</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zack Snyder's Justice League</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soul</td>\n",
       "      <td>PG</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soul</td>\n",
       "      <td>PG</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Gentlemen</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Black Widow</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Suicide Squad</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shang-Chi and the Legend of the Ten Rings</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wonder Woman 1984</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  Rated  review_topic  plot_topic\n",
       "0                                      Tenet  PG-13             0           1\n",
       "1                                       Dune  PG-13             0           1\n",
       "2               Zack Snyder's Justice League      R             0           0\n",
       "3                                       Soul     PG             4           2\n",
       "4                                       Soul     PG             0           2\n",
       "5                              The Gentlemen      R             0           1\n",
       "6                                Black Widow  PG-13             0           1\n",
       "7                          The Suicide Squad      R             0           4\n",
       "8  Shang-Chi and the Legend of the Ten Rings  PG-13             0           0\n",
       "9                          Wonder Woman 1984  PG-13             0           0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data[[\"Title\", \"Rated\", \"review_topic\", \"plot_topic\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.to_csv(\"data/movie_data_withTopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
