{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Complete output from command python setup.py egg_info:\n",
      "    ERROR: Traceback (most recent call last):\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\ruamel_yaml\\__init__.py\", line 21, in <module>\n",
      "        from .main import *                               # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\ruamel_yaml\\main.py\", line 12, in <module>\n",
      "        import ruamel.yaml\n",
      "    ModuleNotFoundError: No module named 'ruamel'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\setup.py\", line 14, in <module>\n",
      "        import ruamel_yaml  # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\ruamel_yaml\\__init__.py\", line 23, in <module>\n",
      "        from ruamel_yaml.main import *                               # NOQA\n",
      "      File \"C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\ruamel_yaml\\main.py\", line 12, in <module>\n",
      "        import ruamel.yaml\n",
      "    ModuleNotFoundError: No module named 'ruamel'\n",
      "    ----------------------------------------\n",
      "ERROR: Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-yw7nm9jc\\ruamel-yaml-conda\\\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NYT Data for 2020 and 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt2020 = pd.read_csv(\"data/NYTData2020.csv\")\n",
    "nyt2021 = pd.read_csv(\"data/NYTData2021.csv\")\n",
    "\n",
    "nyt = nyt2021.append(nyt2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take NYT review summaries and train a Top2Vec Model\n",
    "\n",
    "https://pypi.org/project/top2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 19:38:04,730 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-01-10 19:38:05,084 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:absl:Using C:\\Users\\kenda\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "2022-01-10 19:38:22,116 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2022-01-10 19:38:23,597 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2022-01-10 19:38:52,473 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2022-01-10 19:38:52,640 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "documents = nyt['summary_short'].tolist()\n",
    "\n",
    "model = Top2Vec(documents, speed=\"deep-learn\", workers=8,\n",
    "                embedding_model = \"universal-sentence-encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Number of Topics Defined by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents are most similar to each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([930, 411,  55], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Top Words for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['comedy', 'drama', 'thriller', 'director', 'movie', 'film',\n",
       "        'documentary', 'this', 'netflix', 'into', 'that', 'plays', 'it',\n",
       "        'story', 'the', 'her', 'and', 'as', 'of', 'from', 'its', 'stars',\n",
       "        'but', 'family', 'is', 'with', 'about', 'for', 'an', 'to', 'in',\n",
       "        'feature', 'who', 'his', 'at', 'two', 'new', 'by', 'on',\n",
       "        'follows', 'young', 'are', 'woman', 'life', 'up', 'their'],\n",
       "       ['documentary', 'thriller', 'film', 'drama', 'that', 'this',\n",
       "        'movie', 'comedy', 'about', 'it', 'director', 'life', 'into',\n",
       "        'from', 'story', 'the', 'her', 'of', 'and', 'as', 'but',\n",
       "        'netflix', 'woman', 'who', 'feature', 'with', 'for', 'its', 'an',\n",
       "        'is', 'to', 'on', 'two', 'at', 'by', 'family', 'in', 'up', 'his',\n",
       "        'are', 'plays', 'new', 'stars', 'their', 'young', 'follows'],\n",
       "       ['documentary', 'netflix', 'movie', 'film', 'thriller', 'comedy',\n",
       "        'drama', 'her', 'director', 'this', 'that', 'it', 'as', 'from',\n",
       "        'the', 'its', 'of', 'about', 'feature', 'his', 'but', 'life',\n",
       "        'into', 'woman', 'on', 'with', 'is', 'and', 'for', 'to', 'at',\n",
       "        'story', 'by', 'their', 'an', 'in', 'up', 'two', 'are', 'who',\n",
       "        'stars', 'plays', 'new', 'family', 'young', 'follows']],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with OMDB Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in OMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb2020 = pd.read_csv(\"data/OMDBData2020.csv\")\n",
    "omdb2020.Plot = omdb2020.Plot.fillna(value=\"\")\n",
    "\n",
    "omdb2021 = pd.read_csv(\"data/OMDBData2021.csv\")\n",
    "omdb2021.Plot = omdb2021.Plot.fillna(value=\"\")\n",
    "\n",
    "omdb = omdb2020.append(omdb2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 5k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = (1,2), # include up to 2-grams, we can make this only 1 if needed\n",
    "                                       min_df=1,  # note: absolute count of doc\n",
    "                                       token_pattern = r'\\b[a-z]{3,12}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(omdb[\"Plot\"])\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train NMF for n = 2-10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:\n",
      "[['family', 'young', 'old', 'father', 'year', 'woman', 'year old', 'home', 'mother', 'finds'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'city', 'love']]\n",
      "\n",
      "\n",
      "3 Topics:\n",
      "[['family', 'young', 'old', 'father', 'woman', 'year', 'home', 'year old', 'mother', 'finds'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'city', 'love'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school football', 'school students', 'football']]\n",
      "\n",
      "\n",
      "4 Topics:\n",
      "[['family', 'young', 'father', 'woman', 'home', 'mother', 'son', 'wife', 'daughter', 'house'], ['life', 'new', 'film', 'story', 'world', 'documentary', 'york', 'new york', 'love', 'city'], ['school', 'high', 'high school', 'students', 'team', 'senior', 'student', 'school football', 'school students', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend']]\n",
      "\n",
      "\n",
      "5 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'daughter', 'house'], ['life', 'film', 'story', 'world', 'documentary', 'love', 'american', 'music', 'time', 'people'], ['school', 'high', 'high school', 'students', 'team', 'senior', 'student', 'school football', 'school students', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend'], ['new', 'york', 'new york', 'city', 'york city', 'right', 'swiped', 'time new', 'swiped right', 'based']]\n",
      "\n",
      "\n",
      "6 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'daughter', 'house'], ['story', 'film', 'world', 'love', 'documentary', 'american', 'time', 'follows', 'people', 'music'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school students', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'love', 'old daughter', 'girl', 'best', 'friend'], ['new', 'york', 'new york', 'city', 'york city', 'time new', 'swiped', 'swiped right', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth look', 'depth', 'songwriter', 'singer songwriter']]\n",
      "\n",
      "\n",
      "7 Topics:\n",
      "[['family', 'father', 'young', 'woman', 'mother', 'home', 'son', 'wife', 'house', 'daughter'], ['world', 'film', 'story', 'documentary', 'american', 'time', 'follows', 'history', 'music', 'people'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'student', 'school students', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'best', 'friend', 'thousand year'], ['new', 'york', 'new york', 'city', 'york city', 'swiped right', 'time new', 'swiped', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth', 'depth look', 'songwriter', 'singer songwriter'], ['love', 'films', 'short films', 'oscar nominated', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation']]\n",
      "\n",
      "\n",
      "8 Topics:\n",
      "[['family', 'father', 'young', 'mother', 'woman', 'home', 'son', 'wife', 'daughter', 'house'], ['world', 'film', 'story', 'documentary', 'american', 'time', 'music', 'follows', 'history', 'lives'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'school students', 'student', 'school football', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'best', 'friend', 'thousand year'], ['new', 'york', 'new york', 'city', 'york city', 'swiped right', 'swiped', 'time new', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'singer', 'depth', 'depth look', 'songwriter', 'singer songwriter'], ['love', 'films', 'short films', 'oscar nominated', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['town', 'small', 'small town', 'local', 'midwest', 'midwest town', 'moves', 'death', 'community', 'terrorizing']]\n",
      "\n",
      "\n",
      "9 Topics:\n",
      "[['world', 'town', 'fight', 'small', 'way', 'finds', 'mysterious', 'group', 'time', 'local'], ['film', 'story', 'documentary', 'american', 'music', 'history', 'black', 'people', 'follows', 'world'], ['school', 'high', 'high school', 'students', 'senior', 'team', 'school students', 'school football', 'student', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'thousand year', 'thousand', 'love'], ['new', 'york', 'new york', 'city', 'york city', 'time new', 'swiped', 'swiped right', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'depth', 'depth look', 'singer', 'songwriter', 'singer songwriter'], ['love', 'short films', 'oscar nominated', 'films', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['family', 'home', 'brother', 'new', 'forced', 'trip', 'road', 'mother', 'takes', 'house'], ['young', 'father', 'woman', 'son', 'young woman', 'mother', 'wife', 'daughter', 'girl', 'house']]\n",
      "\n",
      "\n",
      "10 Topics:\n",
      "[['world', 'fight', 'way', 'war', 'time', 'finds', 'mysterious', 'group', 'lives', 'battle'], ['film', 'story', 'documentary', 'american', 'music', 'history', 'black', 'people', 'feature', 'follows'], ['school', 'high school', 'high', 'students', 'senior', 'team', 'school students', 'school football', 'student', 'football'], ['year', 'old', 'year old', 'old girl', 'finds', 'old daughter', 'girl', 'thousand year', 'thousand', 'ground'], ['new', 'york', 'new york', 'city', 'york city', 'swiped', 'swiped right', 'time new', 'right', 'based'], ['life', 'look', 'work', 'look life', 'life work', 'depth', 'depth look', 'singer', 'songwriter', 'singer songwriter'], ['love', 'short films', 'oscar nominated', 'films', 'oscar', 'nominated', 'short', 'nominated short', 'sister', 'animation'], ['family', 'home', 'brother', 'new', 'forced', 'trip', 'road', 'mother', 'takes', 'house'], ['young', 'father', 'woman', 'son', 'young woman', 'mother', 'daughter', 'wife', 'girl', 'house'], ['town', 'small', 'small town', 'midwest', 'local', 'midwest town', 'moves', 'community', 'terrorizing', 'home']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "for num_topics in range(2, 11):\n",
    "    nmf = NMF(\n",
    "        n_components=num_topics,\n",
    "        init='nndsvd', \n",
    "        random_state = 42\n",
    "        )\n",
    "    W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "    H = nmf.components_\n",
    "    top_components = np.argsort(-H)[:, :10]\n",
    "    \n",
    "    topics = []\n",
    "    for t in top_components:\n",
    "        topic_words = []\n",
    "        for w in t:\n",
    "            topic_words.append(feature_names_NMF[w])\n",
    "        topics.append(topic_words)\n",
    "        \n",
    "    print(f\"{num_topics} Topics:\")\n",
    "    print(topics)\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYT Full Review Text\n",
    "\n",
    "TFIDF and NMF on full NYT Review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYTReviews = pd.read_csv(\"data/NYTData_wReviewText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_NMF = TfidfVectorizer(max_features = 20000, # only top 5k by freq\n",
    "                                       lowercase = True, # drop capitalization\n",
    "                                       ngram_range = (1,1), # include up to 2-grams, we can make this only 1 if needed\n",
    "                                       min_df=1,  # note: absolute count of doc\n",
    "                                       token_pattern = r'\\b[a-z]{3,12}\\b',   # remove short, non-word-like terms\n",
    "                                       stop_words='english') # default English stopwords\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(NYTReviews[\"review_text\"])\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:\n",
      "[['movie', 'like', 'story', 'film', 'character', 'time', 'characters', 'family', 'man', 'just'], ['documentary', 'film', 'says', 'footage', 'people', 'new', 'interviews', 'black', 'work', 'camera']]\n",
      "\n",
      "\n",
      "3 Topics:\n",
      "[['movie', 'like', 'story', 'film', 'character', 'time', 'characters', 'family', 'man', 'just'], ['documentary', 'film', 'says', 'footage', 'people', 'black', 'new', 'work', 'interviews', 'life'], ['oscar', 'nominated', 'short', 'program', 'hour', 'colette', 'action', 'live', 'english', 'year']]\n",
      "\n",
      "\n",
      "4 Topics:\n",
      "[['movie', 'like', 'story', 'character', 'characters', 'film', 'time', 'family', 'man', 'just'], ['film', 'documentary', 'says', 'people', 'camera', 'history', 'subjects', 'footage', 'women', 'world'], ['oscar', 'nominated', 'short', 'program', 'hour', 'colette', 'action', 'live', 'english', 'year'], ['music', 'rock', 'musical', 'band', 'new', 'singer', 'artist', 'songs', 'movie', 'documentary']]\n",
      "\n",
      "\n",
      "5 Topics:\n",
      "[['movie', 'woman', 'women', 'film', 'like', 'man', 'story', 'men', 'horror', 'life'], ['documentary', 'film', 'says', 'footage', 'interviews', 'people', 'black', 'history', 'new', 'american'], ['oscar', 'nominated', 'program', 'colette', 'short', 'hour', 'action', 'english', 'live', 'bowers'], ['movie', 'like', 'movies', 'time', 'comedy', 'just', 'played', 'character', 'good', 'directed'], ['family', 'children', 'father', 'mother', 'film', 'child', 'school', 'home', 'son', 'parents']]\n",
      "\n",
      "\n",
      "6 Topics:\n",
      "[['movie', 'woman', 'women', 'like', 'film', 'man', 'story', 'men', 'horror', 'young'], ['documentary', 'film', 'says', 'footage', 'interviews', 'people', 'black', 'history', 'new', 'american'], ['colette', 'oscar', 'bowers', 'latasha', 'burrow', 'nominated', 'short', 'year', 'animation', 'films'], ['movie', 'like', 'movies', 'time', 'comedy', 'just', 'played', 'character', 'good', 'directed'], ['family', 'father', 'children', 'mother', 'film', 'child', 'school', 'home', 'son', 'parents'], ['hour', 'program', 'english', 'filmsnot', 'rated', 'nominated', 'languages', 'oscar', 'live', 'short']]\n",
      "\n",
      "\n",
      "7 Topics:\n",
      "[['film', 'family', 'mother', 'father', 'home', 'life', 'children', 'story', 'movie', 'child'], ['documentary', 'film', 'says', 'people', 'history', 'footage', 'interviews', 'subjects', 'political', 'american'], ['colette', 'oscar', 'bowers', 'latasha', 'burrow', 'nominated', 'short', 'year', 'animation', 'films'], ['movie', 'like', 'character', 'movies', 'story', 'time', 'played', 'just', 'good', 'comedy'], ['music', 'rock', 'band', 'musical', 'singer', 'artist', 'new', 'film', 'songs', 'documentary'], ['deena', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer', 'sam'], ['hour', 'program', 'english', 'filmsnot', 'nominated', 'rated', 'oscar', 'live', 'languages', 'action']]\n",
      "\n",
      "\n",
      "8 Topics:\n",
      "[['film', 'family', 'mother', 'woman', 'life', 'father', 'movie', 'home', 'story', 'man'], ['documentary', 'film', 'says', 'people', 'history', 'footage', 'political', 'interviews', 'american', 'united'], ['colette', 'oscar', 'burrow', 'latasha', 'bowers', 'nominated', 'short', 'year', 'animation', 'films'], ['movie', 'like', 'character', 'movies', 'time', 'played', 'just', 'story', 'good', 'bad'], ['music', 'rock', 'band', 'musical', 'singer', 'artist', 'new', 'songs', 'film', 'documentary'], ['hour', 'program', 'english', 'filmsnot', 'nominated', 'oscar', 'live', 'rated', 'languages', 'action'], ['school', 'kids', 'children', 'boys', 'high', 'girls', 'students', 'film', 'parents', 'girl'], ['deena', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer', 'sam']]\n",
      "\n",
      "\n",
      "9 Topics:\n",
      "[['family', 'father', 'mother', 'children', 'film', 'school', 'home', 'child', 'son', 'parents'], ['documentary', 'film', 'says', 'people', 'history', 'interviews', 'footage', 'subjects', 'american', 'political'], ['colette', 'oscar', 'bowers', 'latasha', 'burrow', 'nominated', 'short', 'year', 'animation', 'entry'], ['movie', 'like', 'played', 'good', 'character', 'comedy', 'time', 'bad', 'movies', 'just'], ['music', 'rock', 'band', 'musical', 'singer', 'artist', 'new', 'film', 'songs', 'documentary'], ['live', 'animated', 'action', 'children', 'short', 'counterparts', 'shorts', 'garden', 'emotional', 'seven'], ['deena', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer', 'sam'], ['woman', 'women', 'movie', 'film', 'like', 'man', 'story', 'young', 'men', 'life'], ['hour', 'program', 'english', 'filmsnot', 'rated', 'languages', 'nominated', 'oscar', 'minutes', 'short']]\n",
      "\n",
      "\n",
      "10 Topics:\n",
      "[['family', 'father', 'mother', 'children', 'film', 'school', 'home', 'child', 'son', 'parents'], ['documentary', 'film', 'says', 'people', 'footage', 'history', 'interviews', 'subjects', 'american', 'states'], ['colette', 'oscar', 'nominated', 'latasha', 'bowers', 'burrow', 'short', 'year', 'animation', 'entry'], ['movie', 'like', 'character', 'good', 'played', 'comedy', 'bad', 'time', 'movies', 'just'], ['music', 'rock', 'band', 'musical', 'singer', 'songs', 'documentary', 'new', 'artist', 'life'], ['live', 'animated', 'action', 'children', 'short', 'counterparts', 'shorts', 'garden', 'emotional', 'seven'], ['deena', 'shadyside', 'trilogy', 'fear', 'sarah', 'street', 'janiak', 'movies', 'summer', 'sam'], ['film', 'movie', 'man', 'woman', 'like', 'life', 'world', 'shot', 'work', 'director'], ['women', 'men', 'black', 'female', 'woman', 'girls', 'story', 'male', 'like', 'husband'], ['hour', 'program', 'english', 'filmsnot', 'rated', 'languages', 'nominated', 'oscar', 'minutes', 'short']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "for num_topics in range(2, 11):\n",
    "    nmf = NMF(\n",
    "        n_components=num_topics,\n",
    "        init='nndsvd', \n",
    "        random_state = 42\n",
    "        )\n",
    "    W = nmf.fit_transform(tfidf_documents_NMF)\n",
    "    H = nmf.components_\n",
    "    top_components = np.argsort(-H)[:, :10]\n",
    "    \n",
    "    topics = []\n",
    "    for t in top_components:\n",
    "        topic_words = []\n",
    "        for w in t:\n",
    "            topic_words.append(feature_names_NMF[w])\n",
    "        topics.append(topic_words)\n",
    "        \n",
    "    print(f\"{num_topics} Topics:\")\n",
    "    print(topics)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
